{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XL_net",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqlX/l5iS1NkuhVjZTewKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingbingodbin/NLP_models/blob/main/XLnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBg14N-WRqy"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTeEn1OvuFKG"
      },
      "source": [
        "import torchtext\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXsNFpb_2nUE",
        "outputId": "65d4b49f-a2fc-4863-cbe1-ef7a78fe03a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36z8vshT7jVN",
        "outputId": "57e504f9-d8a8-4b92-c599-9b6f94ab53ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.listdir('drive/My Drive/datasets/wikitext-2')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wiki.valid.tokens', 'wiki.test.tokens', 'wiki.train.tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXa80D3W5vn",
        "outputId": "756d88af-204a-4bc3-c1db-858b489eb84e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#install package for subword tokenizer\n",
        "!pip install revtok"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2PocX6mWuZn",
        "outputId": "79e98914-9f8c-4374-dabf-c3d064b9c617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_tokenizer(\"revtok\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function revtok.tokenizer.tokenize>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chaubPwz2vjL"
      },
      "source": [
        "dataset_base_path = 'drive/My Drive/datasets/'\n",
        "path = dataset_base_path + 'wikitext-2/'\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"revtok\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntnbNtxN3zF"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmeKUbkw84SV"
      },
      "source": [
        "tr_dat,val_dat,test_dat = torchtext.datasets.language_modeling.WikiText2.splits(TEXT)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRCEMbunX7PO"
      },
      "source": [
        "TEXT.build_vocab(tr_dat)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSkmoEkSUeL"
      },
      "source": [
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(tr_dat, batch_size)\n",
        "val_data = batchify(val_dat, eval_batch_size)\n",
        "test_data = batchify(test_dat, eval_batch_size)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv5OFVg2XxFH",
        "outputId": "9daf2e53-ad38-4ca1-9ff0-991e0533a4dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "[tr_dat.examples[0].text][0][4000:4800]"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' along ',\n",
              " ' the ',\n",
              " ' same ',\n",
              " ' timeline ',\n",
              " ' as ',\n",
              " ' the ',\n",
              " ' original ',\n",
              " ' game ',\n",
              " ' and ',\n",
              " ' its ',\n",
              " ' television ',\n",
              " ' anime ',\n",
              " ' adaptation ',\n",
              " ' , ',\n",
              " ' the ',\n",
              " ' cast ',\n",
              " ' of ',\n",
              " ' valkyria ',\n",
              " ' chronicles ',\n",
              " ' could ',\n",
              " ' make ',\n",
              " ' appearances ',\n",
              " ' , ',\n",
              " ' which ',\n",
              " ' pleased ',\n",
              " ' the ',\n",
              " ' team ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' opening ',\n",
              " ' theme ',\n",
              " ' , ',\n",
              " ' \" ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ( ',\n",
              " ' light ',\n",
              " ' ) ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' \" ',\n",
              " ' ( ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' @ ',\n",
              " '- ',\n",
              " '@ ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ) ',\n",
              " ' , ',\n",
              " ' was ',\n",
              " ' sung ',\n",
              " ' by ',\n",
              " ' japanese ',\n",
              " ' singer ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' ending ',\n",
              " ' theme ',\n",
              " ' , ',\n",
              " ' \" ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' the ',\n",
              " ' flowers ',\n",
              " ' of ',\n",
              " ' light ',\n",
              " ' will ',\n",
              " ' bloom ',\n",
              " ' \" ',\n",
              " ' ( ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' , ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' no ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ) ',\n",
              " ' , ',\n",
              " ' was ',\n",
              " ' sung ',\n",
              " ' by ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' . ',\n",
              " ' both ',\n",
              " ' songs ',\n",
              " \" ' \",\n",
              " ' lyrics ',\n",
              " ' were ',\n",
              " ' written ',\n",
              " ' by ',\n",
              " ' their ',\n",
              " ' respective ',\n",
              " ' artists ',\n",
              " ' . ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' two ',\n",
              " ' manga ',\n",
              " ' adaptations ',\n",
              " ' were ',\n",
              " ' produced ',\n",
              " ' , ',\n",
              " ' following ',\n",
              " ' each ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' game ',\n",
              " \" '\",\n",
              " ' s ',\n",
              " ' main ',\n",
              " ' female ',\n",
              " ' protagonists ',\n",
              " ' imca ',\n",
              " ' and ',\n",
              " ' riela ',\n",
              " ' . ',\n",
              " ' they ',\n",
              " ' were ',\n",
              " ' senjō ',\n",
              " ' no ',\n",
              " ' valkyria ',\n",
              " ' 3 ',\n",
              " ' : ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' no ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ( ',\n",
              " ' 戦場のヴァルキュリア ',\n",
              " '3 ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' , ',\n",
              " ' lit ',\n",
              " ' . ',\n",
              " ' valkyria ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' battlefield ',\n",
              " ' 3 ',\n",
              " ' : ',\n",
              " ' the ',\n",
              " ' flower ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' nameless ',\n",
              " ' oath ',\n",
              " ' ) ',\n",
              " ' , ',\n",
              " ' illustrated ',\n",
              " ' by ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' and ',\n",
              " ' eventually ',\n",
              " ' released ',\n",
              " ' in ',\n",
              " ' two ',\n",
              " ' volumes ',\n",
              " ' after ',\n",
              " ' being ',\n",
              " ' serialized ',\n",
              " ' in ',\n",
              " ' dengeki ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' between ',\n",
              " ' 2011 ',\n",
              " ' and ',\n",
              " ' 2012 ',\n",
              " ' ; ',\n",
              " ' and ',\n",
              " ' senjō ',\n",
              " ' no ',\n",
              " ' valkyria ',\n",
              " ' 3 ',\n",
              " ' : ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' no ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ( ',\n",
              " ' 戦場のヴァルキュリア ',\n",
              " '3 ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' , ',\n",
              " ' lit ',\n",
              " ' . ',\n",
              " ' valkyria ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' battlefield ',\n",
              " ' 3 ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' crimson ',\n",
              " ' fate ',\n",
              " ' ) ',\n",
              " ' , ',\n",
              " ' illustrated ',\n",
              " ' by ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' and ',\n",
              " ' eventually ',\n",
              " ' released ',\n",
              " ' in ',\n",
              " ' a ',\n",
              " ' single ',\n",
              " ' volume ',\n",
              " ' by ',\n",
              " ' kadokawa ',\n",
              " ' shoten ',\n",
              " ' in ',\n",
              " ' 2012 ',\n",
              " ' . ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' = ',\n",
              " ' tower ',\n",
              " ' building ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' arsenal ',\n",
              " ' = ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' the ',\n",
              " ' tower ',\n",
              " ' building ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' arsenal ',\n",
              " ' , ',\n",
              " ' also ',\n",
              " ' known ',\n",
              " ' as ',\n",
              " ' u ',\n",
              " '.',\n",
              " ' s ',\n",
              " '. ',\n",
              " ' arsenal ',\n",
              " ' building ',\n",
              " ' , ',\n",
              " ' is ',\n",
              " ' a ',\n",
              " ' building ',\n",
              " ' located ',\n",
              " ' in ',\n",
              " ' macarthur ',\n",
              " ' park ',\n",
              " ' in ',\n",
              " ' downtown ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' , ',\n",
              " ' arkansas ',\n",
              " ' . ',\n",
              " ' built ',\n",
              " ' in ',\n",
              " ' 1840 ',\n",
              " ' , ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' part ',\n",
              " ' of ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " \" '\",\n",
              " ' s ',\n",
              " ' first ',\n",
              " ' military ',\n",
              " ' installation ',\n",
              " ' . ',\n",
              " ' since ',\n",
              " ' its ',\n",
              " ' decommissioning ',\n",
              " ' , ',\n",
              " ' the ',\n",
              " ' tower ',\n",
              " ' building ',\n",
              " ' has ',\n",
              " ' housed ',\n",
              " ' two ',\n",
              " ' museums ',\n",
              " ' . ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' home ',\n",
              " ' to ',\n",
              " ' the ',\n",
              " ' arkansas ',\n",
              " ' museum ',\n",
              " ' of ',\n",
              " ' natural ',\n",
              " ' history ',\n",
              " ' and ',\n",
              " ' antiquities ',\n",
              " ' from ',\n",
              " ' 1942 ',\n",
              " ' to ',\n",
              " ' 1997 ',\n",
              " ' and ',\n",
              " ' the ',\n",
              " ' macarthur ',\n",
              " ' museum ',\n",
              " ' of ',\n",
              " ' arkansas ',\n",
              " ' military ',\n",
              " ' history ',\n",
              " ' since ',\n",
              " ' 2001 ',\n",
              " ' . ',\n",
              " ' it ',\n",
              " ' has ',\n",
              " ' also ',\n",
              " ' been ',\n",
              " ' the ',\n",
              " ' headquarters ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' æsthetic ',\n",
              " ' club ',\n",
              " ' since ',\n",
              " ' 1894 ',\n",
              " ' . ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' the ',\n",
              " ' building ',\n",
              " ' receives ',\n",
              " ' its ',\n",
              " ' name ',\n",
              " ' from ',\n",
              " ' its ',\n",
              " ' distinct ',\n",
              " ' octagonal ',\n",
              " ' tower ',\n",
              " ' . ',\n",
              " ' besides ',\n",
              " ' being ',\n",
              " ' the ',\n",
              " ' last ',\n",
              " ' remaining ',\n",
              " ' structure ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' original ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' arsenal ',\n",
              " ' and ',\n",
              " ' one ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' oldest ',\n",
              " ' buildings ',\n",
              " ' in ',\n",
              " ' central ',\n",
              " ' arkansas ',\n",
              " ' , ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' also ',\n",
              " ' the ',\n",
              " ' birthplace ',\n",
              " ' of ',\n",
              " ' general ',\n",
              " ' douglas ',\n",
              " ' macarthur ',\n",
              " ' , ',\n",
              " ' who ',\n",
              " ' became ',\n",
              " ' the ',\n",
              " ' supreme ',\n",
              " ' commander ',\n",
              " ' of ',\n",
              " ' us ',\n",
              " ' forces ',\n",
              " ' in ',\n",
              " ' the ',\n",
              " ' south ',\n",
              " ' pacific ',\n",
              " ' during ',\n",
              " ' world ',\n",
              " ' war ',\n",
              " ' ii ',\n",
              " ' . ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' also ',\n",
              " ' the ',\n",
              " ' starting ',\n",
              " ' place ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' camden ',\n",
              " ' expedition ',\n",
              " ' . ',\n",
              " ' in ',\n",
              " ' 2011 ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' named ',\n",
              " ' as ',\n",
              " ' one ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' top ',\n",
              " ' 10 ',\n",
              " ' attractions ',\n",
              " ' in ',\n",
              " ' the ',\n",
              " ' state ',\n",
              " ' of ',\n",
              " ' arkansas ',\n",
              " ' by ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' = ',\n",
              " ' = ',\n",
              " ' construction ',\n",
              " ' = ',\n",
              " ' = ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' the ',\n",
              " ' arsenal ',\n",
              " ' was ',\n",
              " ' constructed ',\n",
              " ' at ',\n",
              " ' the ',\n",
              " ' request ',\n",
              " ' of ',\n",
              " ' governor ',\n",
              " ' james ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' conway ',\n",
              " ' in ',\n",
              " ' response ',\n",
              " ' to ',\n",
              " ' the ',\n",
              " ' perceived ',\n",
              " ' dangers ',\n",
              " ' of ',\n",
              " ' frontier ',\n",
              " ' life ',\n",
              " ' and ',\n",
              " ' fears ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' many ',\n",
              " ' native ',\n",
              " ' americans ',\n",
              " ' who ',\n",
              " ' were ',\n",
              " ' passing ',\n",
              " ' through ',\n",
              " ' the ',\n",
              " ' state ',\n",
              " ' on ',\n",
              " ' their ',\n",
              " ' way ',\n",
              " ' to ',\n",
              " ' the ',\n",
              " ' newly ',\n",
              " ' established ',\n",
              " ' oklahoma ',\n",
              " ' territory ',\n",
              " ' . ',\n",
              " ' thirty ',\n",
              " ' @ ',\n",
              " '- ',\n",
              " '@ ',\n",
              " ' six ',\n",
              " ' acres ',\n",
              " ' were ',\n",
              " ' appropriated ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' outskirts ',\n",
              " ' of ',\n",
              " ' little ',\n",
              " ' rock ',\n",
              " ' by ',\n",
              " ' major ',\n",
              " ' robert ',\n",
              " ' b ',\n",
              " '. ',\n",
              " ' lee ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' u ',\n",
              " '.',\n",
              " ' s ',\n",
              " '. ',\n",
              " ' army ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' land ',\n",
              " ' had ',\n",
              " ' been ',\n",
              " ' previously ',\n",
              " ' used ',\n",
              " ' as ',\n",
              " ' a ',\n",
              " ' racetrack ',\n",
              " ' by ',\n",
              " ' the ',\n",
              " ' local ',\n",
              " ' jockey ',\n",
              " ' club ',\n",
              " ' . ',\n",
              " ' john ',\n",
              " ' <',\n",
              " ' unk ',\n",
              " '> ',\n",
              " ' walker ',\n",
              " ' , ',\n",
              " ' a ',\n",
              " ' builder ',\n",
              " ' for ',\n",
              " ' the ',\n",
              " ' federal ',\n",
              " ' government ',\n",
              " ' , ',\n",
              " ' supervised ',\n",
              " ' the ',\n",
              " ' construction ',\n",
              " ' . ',\n",
              " ' originally ',\n",
              " ' $ ',\n",
              " ' 14 ',\n",
              " ' @ ',\n",
              " ', ',\n",
              " '@ ',\n",
              " ' 000 ',\n",
              " ' was ',\n",
              " ' allocated ',\n",
              " ' for ',\n",
              " ' the ',\n",
              " ' construction ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' arsenal ',\n",
              " ' , ',\n",
              " ' but ',\n",
              " ' proved ',\n",
              " ' inadequate ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' budget ',\n",
              " ' was ',\n",
              " ' later ',\n",
              " ' increased ',\n",
              " ' to ',\n",
              " ' $ ',\n",
              " ' 30 ',\n",
              " ' @ ',\n",
              " ', ',\n",
              " '@ ',\n",
              " ' 000 ',\n",
              " ' . ',\n",
              " ' work ',\n",
              " ' began ',\n",
              " ' on ',\n",
              " ' the ',\n",
              " ' tower ',\n",
              " ' building ',\n",
              " ' in ',\n",
              " ' 1840 ',\n",
              " ' , ',\n",
              " ' and ',\n",
              " ' it ',\n",
              " ' was ',\n",
              " ' the ',\n",
              " ' first ',\n",
              " ' permanent ',\n",
              " ' structure ',\n",
              " ' of ',\n",
              " ' the ',\n",
              " ' arsenal ',\n",
              " ' to ',\n",
              " ' be ',\n",
              " ' built ',\n",
              " ' . ',\n",
              " ' being ',\n",
              " ' originally ',\n",
              " ' constructed ',\n",
              " ' to ',\n",
              " ' store ',\n",
              " ' ammunition ',\n",
              " ' , ',\n",
              " ' the ',\n",
              " ' building ',\n",
              " ' was ',\n",
              " ' designed ',\n",
              " ' with ',\n",
              " ' 3 ',\n",
              " ' @ ',\n",
              " '- ',\n",
              " '@ ',\n",
              " ' foot ',\n",
              " ' @ ',\n",
              " '- ',\n",
              " '@ ',\n",
              " ' thick ',\n",
              " ' ( ',\n",
              " ' 0 ',\n",
              " ' @ ',\n",
              " '. ',\n",
              " '@ ',\n",
              " ' 91 ',\n",
              " ' m ',\n",
              " ' ) ',\n",
              " ' exterior ',\n",
              " ' walls ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' original ',\n",
              " ' plans ',\n",
              " ' called ',\n",
              " ' for ',\n",
              " ' it ',\n",
              " ' to ',\n",
              " ' be ',\n",
              " ' built ',\n",
              " ' of ',\n",
              " ' stone ',\n",
              " ' , ',\n",
              " ' however ',\n",
              " ' , ',\n",
              " ' masonry ',\n",
              " ' was ',\n",
              " ' used ',\n",
              " ' instead ',\n",
              " ' . ',\n",
              " ' the ',\n",
              " ' arkansas ',\n",
              " ' gazette ',\n",
              " ' referred ',\n",
              " ' to ',\n",
              " ' the ',\n",
              " ' structure ',\n",
              " ' as ',\n",
              " ' \" ',\n",
              " ' a ',\n",
              " ' splendid ',\n",
              " ' specimen ',\n",
              " ' of ',\n",
              " ' masonry ',\n",
              " ' \" ',\n",
              " ' . ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' = ',\n",
              " ' = ',\n",
              " ' civil ',\n",
              " ' war ',\n",
              " ' = ',\n",
              " ' = ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' for ',\n",
              " ' several ',\n",
              " ' years ',\n",
              " ' the ',\n",
              " ' arsenal ',\n",
              " ' , ',\n",
              " ' which ',\n",
              " ' was ',\n",
              " ' owned ',\n",
              " ' by ',\n",
              " ' the ',\n",
              " ' federal ',\n",
              " ' government ',\n",
              " ' , ',\n",
              " ' served ',\n",
              " ' as ',\n",
              " ' a ',\n",
              " ' simple ',\n",
              " ' arms ',\n",
              " ' depot ',\n",
              " ' and ',\n",
              " ' was ',\n",
              " ' staffed ',\n",
              " ' with ',\n",
              " ' only ',\n",
              " ' a ',\n",
              " ' handful ',\n",
              " ' of ',\n",
              " ' soldiers ',\n",
              " ' . ',\n",
              " ' but ',\n",
              " ' in ',\n",
              " ' november ',\n",
              " ' 1860 ',\n",
              " ' , ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMDNrB8iYRjg",
        "outputId": "3656384b-d915-4a69-d3f1-f7d9f982dbe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,    29,  4031,  2997,   447,  3285,     4,   133,   444,    36,\n",
              "         3745,     3,   838,     6, 24073,  1484,     4,    25, 13603,    71])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKr0gTe1ZL5B",
        "outputId": "0421253f-8def-4ee5-bcac-c2a75ceefc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,  1781,    20,   179, 15130,   765,   632,    59,   490,  6997,\n",
              "           27,     7,     6,     4,   480,  5051,    60,    15,     8,     9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvk-s2rAYE3F",
        "outputId": "775b1efe-fc98-43be-aca6-bcc159a2c1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "TEXT."
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-148-97a158e07648>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    TEXT.vocab.()\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt9bcA38QVly"
      },
      "source": [
        "en_textfield.build_vocab(tr_dat, min_freq = 2)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usTUPO0SSHH0"
      },
      "source": [
        ""
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RRFhrAJuhLT"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (tr_dat, val_dat, test_dat),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpaDCwOuJS6"
      },
      "source": [
        "for i,batch in train_iterator:\n",
        "  aa = batch\n",
        "  bb = i"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65YpZ6lRctRT",
        "outputId": "d35f9ac9-d473-41b0-a6d6-8a19fd8ebd2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bb"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   9,   11, 3875,  ...,    4,    9,    9]]), tensor([2088628]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjRwKvDVZE6"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvAhDzXVo4x"
      },
      "source": [
        "ds = tfds.load('glue', split='train', as_supervised=False, shuffle_files=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGywEswRWqYw",
        "outputId": "d6a25df3-bf9a-46fd-bbf6-4935f1c62d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "for i in ds:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cc2ab6e2e158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1LN41CIVcxW",
        "outputId": "9891165d-ad56-4226-983b-94fd97eea533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "aa.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-603b00bbb490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Glue' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA54tjsGB73e",
        "outputId": "615a630b-f343-43e0-ef74-bdd72e2cf185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "np.array([1,2,3]).shape()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a69473d3b56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFIb0Ta-9pt"
      },
      "source": [
        "ddd = aa(a = np.array([1,2,4,5]),b=np.array([1,2,4,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZZQirEK_0-H"
      },
      "source": [
        "exec('b=ddd.a.sum()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "norvzpPi_0Hm",
        "outputId": "94505df7-9556-490d-91cc-cee5ed60ee07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(np.array([1,2,3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8neGAK_MSV",
        "outputId": "59268c2b-827c-4152-aa90-5485f0d97107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "sample_dict.get('aa')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qisDhWYI_Agt"
      },
      "source": [
        "sample_dict = {'aa': 12, 'bb':13}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3gvdyAs-6mN",
        "outputId": "176553f5-8ace-4c86-abde-e8c0fea3275a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "'list is list'.split(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'is', 'list']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghK98IUDdOeV"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c2UthJ4dXb7"
      },
      "source": [
        "#활용된 잡지식\n",
        "\n",
        "#pytorch dataset 불러오기 \n",
        "#https://pytorch.org/text/datasets.html\n",
        "\n",
        "#자동으로 추가해야할 내용들\n",
        "#To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference\n",
        "\n",
        "#데이터 로딩\n",
        "#data loading\n",
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
        "\n",
        "#함수 오버라이딩 방법\n",
        "#super(MyModel, self).__init__()\n",
        "\n",
        "#einstein summation\n",
        "#https://rockt.github.io/2018/04/30/einsum\n",
        "\n",
        "#defining method with string\n",
        "#https://stackoverflow.com/questions/11553721/using-a-string-variable-as-a-variable-name\n",
        "\n",
        "#kwargs,args\n",
        "#https://brunch.co.kr/@princox/180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLLgpI56dZnK"
      },
      "source": [
        "# Two-stream attention cell\n",
        "#input : q_size, k_size, v_size, segment_index, attention_size, heads =8, **kwargs = {init_Qstream_q_LT_weight, init_Qstream_q_K_weight,init_Qstream_q_R_weight, init_Qstream_k_E_LT_weight, init_Qstream_k_R_LT_weight, init_Qstream_v_LT_weight,\n",
        "#                                                                            init_Qstream_segment_same_weight, init_Qstream_segment_notsame_weight, init_Qstream_segment_bias, init_Qstream_output_weight,\n",
        "#                                                                            init_Cstream_q_LT_weight, init_Cstream_q_K_weight,init_Cstream_q_R_weight, init_Cstream_k_E_LT_weight, init_Cstream_k_R_LT_weight, init_Cstream_v_LT_weight,\n",
        "#                                                                            init_Cstream_segment_same_weight, init_Cstream_segment_notsame_weight, init_Cstream_segment_bias, init_Cstream_output_weight}\n",
        "#\n",
        "\n",
        "#operation : q_LT, k_LT, v_LT, segmentation, permutation(optional), masking, relative_position_encoding, segment_encoding, cal_content_attention, cal_query_attention, memorise\n",
        "\n",
        "#properties : Qstream_q_LT_weight, Qstream_q_K_weight,Qstream_q_R_weight, Qstream_k_E_LT_weight, Qstream_k_R_LT_weight, Qstream_v_LT_weight,\n",
        "#             Qstream_segment_same_weight, Qstream_segment_notsame_weight, Qstream_segment_bias, Qstream_output_weight,\n",
        "#             Cstream_q_LT_weight, Cstream_q_K_weight,Cstream_q_R_weight, Cstream_k_E_LT_weight, Cstream_k_R_LT_weight, Cstream_v_LT_weight,\n",
        "#             Cstream_segment_same_weight, Cstream_segment_notsame_weight, Cstream_segment_bias, Cstream_output_weight content_attention, query_attention, content_memory, query_memory\n",
        "\n",
        "#output : Dq vector(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9SPAirbqb1a"
      },
      "source": [
        "class create_PLM_input(data_utils.Dataset):\n",
        "  def __init__(self, data, target_csv, word_dict = False, sep_by_lines = False, segment_len = False, do_shuffle=True):\n",
        "    super(create_PLM_input, self).__init__()\n",
        "    \n",
        "    self.target_csv = pd.read_csv(traget_csv)\n",
        "    self.dat_shape = data.shape\n",
        "    self.root_dir = root_dir\n",
        "    self.segment_len = segment_len\n",
        "    self.word_dict = False\n",
        "    self.data = data\n",
        "\n",
        "    if do_shuffle:\n",
        "      shuffle()\n",
        "    \n",
        "    segmentation()\n",
        "\n",
        "  def segmentation(self):\n",
        "    self.segments = \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    doc_paths = os.path.join(self.root_dir, self.target_csv.iloc[idx, 0])\n",
        "    for doc_path in doc_paths\n",
        "    doc_file = open(doc_path, 'r')\n",
        "\n",
        "    lines = doc_file.readlines()\n",
        "\n",
        "    if word_dict:\n",
        "      for line in lines:\n",
        "        for \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    doc_file.close()\n",
        "\n",
        "\n",
        "    f = open(\"C:/doit/새파일.txt\", 'r')\n",
        "\n",
        "\n",
        "      landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "      landmarks = np.array([landmarks])\n",
        "      landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "      sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4950byzWJBsJ"
      },
      "source": [
        "class create_PLM_input_inmemory(data_utils.Dataset):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edo49bgpmrCX",
        "outputId": "e768e5b7-29bf-443e-b8cc-16209fbd144f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perred\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V_hdr4sDpfz"
      },
      "source": [
        "a= torch.nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS8PYlKKEmbS"
      },
      "source": [
        "def shuffle()\n",
        "\n",
        "def permutate()\n",
        "\n",
        "def rel_position_encoding():\n",
        "\n",
        "def masking():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Fa0rBOEpbf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQmmsxUrmp8q"
      },
      "source": [
        "if not a:\n",
        "  print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHFApw4UPpar",
        "outputId": "6a74a673-3317-4367-fcb9-72efeaa73f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "class Two_stream_attention(nn):\n",
        "  def __init__(self, q_size, k_size, v_size, attention_size = False, heads =8, **kwargs):\n",
        "    super(Two_stream_attention, self).__init__() \n",
        "      \n",
        "\n",
        "    self.segment_index = segment_index\n",
        "\n",
        "    #initialize params\n",
        "    self.initialize_params(self, q_size, k_size, v_size, attention_size, heads)\n",
        "\n",
        "  def initialize_params(self, q_size, k_size, v_size, attention_size, heads):\n",
        "    self.Qstream_q_LT_weight = nn.Linear()\n",
        "    self.Qstream_q_K_weight = \n",
        "    self.Qstream_q_R_weight = \n",
        "    \n",
        "    self.Qstream_k_E_LT_weight = \n",
        "    self.Qstream_k_R_LT_weight = \n",
        "    self.Qstream_v_LT_weight = \n",
        "    \n",
        "    self.Qstream_segment_same_weight = \n",
        "    self.Qstream_segment_notsame_weight = \n",
        "    self.Qstream_segment_bias = \n",
        "    \n",
        "    self.Qstream_output_weight = \n",
        "\n",
        "    self.Cstream_q_LT_weight = \n",
        "    self.Cstream_q_K_weight = \n",
        "    self.Cstream_q_R_weight = \n",
        "    \n",
        "    self.Cstream_k_E_LT_weight = \n",
        "    self.Cstream_k_R_LT_weight = \n",
        "    self.Cstream_v_LT_weight = \n",
        "\n",
        "    self.Cstream_segment_same_weight = \n",
        "    self.Cstream_segment_notsame_weight = \n",
        "    self.Cstream_segment_bias = \n",
        "    \n",
        "    self.Cstream_output_weight =\n",
        "\n",
        "  def set_params(self,**kwargs):\n",
        "    for i,j in kwargs.items():\n",
        "      setattr(self, i, j)\n",
        "      explicitly_defined_vars.append(i)\n",
        "\n",
        "  def forward(self, input, do_parm = False, rel_position = False):\n",
        "    \n",
        "    #record position\n",
        "    if not rel_position:\n",
        "      self.rel_position = rel_position_encoding(input)\n",
        "    else:\n",
        "      self.rel_position = rel_position\n",
        "\n",
        "    #permutation\n",
        "    if do_parm:\n",
        "      self.permutate()\n",
        "\n",
        "    #get position encoding\n",
        "\n",
        "    #cal\n",
        "  def backward(self,??): #클레스 정의하는 법 참조\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ad5720c06428>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    self.Qstream_q_K_weight =\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laXkR4scazxZ"
      },
      "source": [
        "_#http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxPfane-X3-A"
      },
      "source": [
        "##Copyright 2013, Youngbin Jang, All rights reserved."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}