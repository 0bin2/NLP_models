{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XL_net",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwA/lM1Kje3xPKksbyX0lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingbingodbin/NLP_models/blob/main/XLnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBg14N-WRqy"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from itertools import combinations"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTeEn1OvuFKG"
      },
      "source": [
        "import torchtext\n",
        "import io"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXsNFpb_2nUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876bc423-4565-4903-d5a3-779b6821660a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36z8vshT7jVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359bcc23-da8a-4cf3-c74c-dd332ae71237"
      },
      "source": [
        "import os\n",
        "os.listdir('drive/My Drive/datasets/wikitext-2')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wiki.valid.tokens', 'wiki.test.tokens', 'wiki.train.tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXa80D3W5vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cc63a0-247c-47cf-ded8-c3ae5a715ba9"
      },
      "source": [
        "#install package for subword tokenizer\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To7FLwGYVLvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5101bd2d-b514-42a7-d11d-2209e9f68b72"
      },
      "source": [
        "#install package for subword tokenizer\n",
        "!pip install revtok"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZiU1OptMLU"
      },
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2PocX6mWuZn"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xItrfu9Tc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a5aa15-a049-46bb-8b65-8933499fcbb5"
      },
      "source": [
        "import urllib.request\n",
        "import io\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Loads model from URL as iterator and stores the model to BytesIO.\n",
        "model = io.BytesIO()\n",
        "with urllib.request.urlopen(\n",
        "    'https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt'\n",
        ") as response:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      sentence_iterator=response, model_writer=model, vocab_size=1000)\n",
        "\n",
        "# Serialize the model as file.\n",
        "# with open('out.model', 'wb') as f:\n",
        "#   f.write(model.getvalue())\n",
        "\n",
        "# Directly load the model from serialized model.\n",
        "sp = spm.SentencePieceProcessor(model_proto=model.getvalue())\n",
        "print(sp.encode('this is test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[64, 47, 4, 13, 391]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoVHOO1x9WvS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "a9ef3262-b6ea-4eaf-e928-6f802d9eb4cf"
      },
      "source": [
        "aa = torchtext.datasets.language_modeling.WikiText2.splits()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f7f4ebfdd56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_modeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWikiText2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: splits() missing 1 required positional argument: 'text_field'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GskmwEYG97uF"
      },
      "source": [
        "\"\"\"load wikitext2 data\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxS-wshN0_VC"
      },
      "source": [
        "tokenizer = get_tokenizer('revtok')"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFMXiEoE1CQW"
      },
      "source": [
        "from revtok import tokenizer"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "CmWK4p4Z1iiN",
        "outputId": "28bf3de8-40a2-411b-e5d6-d9590819fe25"
      },
      "source": [
        ""
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-c6ed24591afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'revtok.tokenizer' has no attribute 'add_special_token'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chaubPwz2vjL"
      },
      "source": [
        "dataset_base_path = 'drive/My Drive/datasets/'\n",
        "path = dataset_base_path + 'wikitext-2/'\n",
        "#use reversable tokenizer for flexible \n",
        "TEXT = torchtext.data.ReversibleField(tokenize = get_tokenizer(\"revtok\"),\n",
        "                            init_token = '<sos>',\n",
        "                            eos_token = '<eos>',\n",
        "                            unk_token ='<unk>',\n",
        "                            sequential=True,\n",
        "                            lower=True,\n",
        "                            use_vocab=True)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntnbNtxN3zF"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmeKUbkw84SV"
      },
      "source": [
        "tr_dat,val_dat,te_dat = torchtext.datasets.language_modeling.WikiText2.splits(TEXT)"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSkmoEkSUeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9615ed17-18a9-4bb3-c509-fecf937366ed"
      },
      "source": [
        "[tr_dat.examples[0].text][0][:10]"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' = ',\n",
              " ' valkyria ',\n",
              " ' chronicles ',\n",
              " ' iii ',\n",
              " ' = ',\n",
              " ' ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n6QXjlrVet_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43412ef-87bb-4ff4-907b-7b0009391af3"
      },
      "source": [
        "[val_dat.examples[0].text][0][:10]"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ', '<eos>', ' ', ' = ', ' homarus ', ' gammarus ', ' = ', ' ', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01iBnn8Vh5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a502e0-fe08-4737-b74c-8054d053786f"
      },
      "source": [
        "[te_dat.examples[0].text][0][:10]"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ', '<eos>', ' ', ' = ', ' robert ', ' <', ' unk ', '> ', ' = ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSlLqon-6F0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c813ab-83da-4da4-ea71-f3de4f694b18"
      },
      "source": [
        "hasattr(TEXT, \"vocab\")"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFqSs_Ftg7Fd"
      },
      "source": [
        "#seperating wikitext2 data to documents\n",
        "def sep_to_docs(data : list, doc_sep : str, token_sep : str) -> list:\n",
        "  \"\"\"returns documents from split input text chunk\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  docs = (''.join(data)).split(doc_sep)\n",
        "  for i in docs:\n",
        "    curr_doc = (''.join(i)).split(token_sep)\n",
        "    curr_doc = [i for i in curr_doc if i != '']\n",
        "    if len(curr_doc) >= 1:\n",
        "      result.append(curr_doc)\n",
        "  return result"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rilTTz4oroyR"
      },
      "source": [
        "tr_docs = sep_to_docs(data = [tr_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')\n",
        "val_docs = sep_to_docs(data = [val_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')\n",
        "te_docs = sep_to_docs(data = [te_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Kl4dHcsH34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450a0138-2ab9-43b8-9a20-31fbfde164fa"
      },
      "source": [
        "print(len(tr_docs),len(val_docs),len(te_docs))"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6211 620 708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82N7GhvPWqho"
      },
      "source": [
        "#create vocab with glove.6B.100d\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10,vectors=\"glove.6B.100d\")\n",
        "#!cp .vector_cache/glove.6B.zip drive/\"My Drive\"/datasets/\n",
        "\n",
        "\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10)"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qznBPpMilCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ddb0581-23bf-4732-a5d6-be08c7dc6ec3"
      },
      "source": [
        "\"\"\"extracting vocab\"\"\"\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10,vectors=\"glove.6B.100d\")\n",
        "#!cp .vector_cache/glove.6B.zip drive/\"My Drive\"/datasets/\n",
        "#glove_zip = zipfile.ZipFile('drive/My Drive/datasets/glove.6B.zip')\n",
        "#glove_zip.extractall('drive/My Drive/datasets/pretrained_wordvectors')\n",
        "#glove_zip.close()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'extracting vocab'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze4U_d__iEdv"
      },
      "source": [
        "from torchtext.vocab import Vectors\n",
        "glove_6b_300d_vec = Vectors(name='glove.6B.300d.txt', cache='drive/My Drive/datasets/pretrained_wordvectors')\n",
        "\n",
        "special_tokens = '<cls>'\n",
        "TEXT.build_vocab(tr_docs, min_freq=10, vectors=glove_6b_300d_vec)"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e4cFwC_tG4",
        "outputId": "71ad0d53-8789-46f9-c11e-b1c3236c214a"
      },
      "source": [
        ""
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.vocab.Vocab"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw-kEuavWz8z"
      },
      "source": [
        "#<cls>, <sep> 추가\n",
        "\n",
        "TEXT.build_vocab([['<cls>','<sep>']])"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeDGAHX0-dyj",
        "outputId": "e78be74a-efb3-4614-9276-60d709974cda"
      },
      "source": [
        "TEXT.vocab.vectors.shape"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12983, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skVlrr6m9Ggt"
      },
      "source": [
        "aa = torchtext.data.Field()\n",
        "aa.build_vocab([['<cls>','<sep>']])"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZP8kR9v-xYa"
      },
      "source": [
        "TEXT.vocab.extend(aa.vocab)"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "ATJ-R8RdRxwU",
        "outputId": "4217fffc-30a0-467a-f3b7-c808eafe8978"
      },
      "source": [
        "TEXT.build_vocab(tr_docs, min_freq=10, specials=['<sep>', '<cls>'])"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-341-69b85cfcc710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<sep>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<cls>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                             self.eos_token]\n\u001b[1;32m    272\u001b[0m             if tok is not None))\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: type object got multiple values for keyword argument 'specials'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNkbFZLcm-jz"
      },
      "source": [
        "vocab_size = len(TEXT.vocab.itos)\n",
        "word_embedding_size = TEXT.vocab.vectors.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV69mA7wXiQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "a23654df-d12a-4e99-dd18-7853c05ee443"
      },
      "source": [
        "#sample embedding\n",
        "embeds = nn.Embedding(vocab_size, word_embedding_size)\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-f233c12bee7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sample embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 2 words in vocab, 5 dimensional embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlookup_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhello_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhello_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_to_ix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_KbWrmdcF_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc9bfc7-64e7-41bf-e0b4-b2cddfa44e03"
      },
      "source": [
        "a=[]\n",
        "a.append([1,2,3])\n",
        "a"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL94NrpNbZhJ"
      },
      "source": [
        "TEXT.batch_first=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4muV-7IkZeG",
        "outputId": "634a2987-943e-48b5-a808-fd2cd0719aca"
      },
      "source": [
        "for i in combinations([1,2,3],2):\n",
        "  if self.doc_index[i[0-1]:i[0]] != self.doc_index[i[0]]:\n",
        "    \n",
        "  else :\n",
        "    (aa[i[0-1]:i[0]],'<cls>',aa[i[0]],'<sep>',aa[i[1]])\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (1, 3), (2, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJUz8Uthzdar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1c2d43-7636-472b-a7ea-219329bced19"
      },
      "source": [
        "[1,2,3] +[[1]]"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, [1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8UK57dy1vvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e82f2f5-00a4-453a-c18e-54fb185acdf1"
      },
      "source": [
        "torch.tensor(list(range(len([1,2,3,4]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mVC_u1I0wW"
      },
      "source": [
        "def build_Field_with_vocab(text_dat: list, Field: torchtext.data.Field, special_tokens: list=None, min_freq: int=10, learned_vectors: Vectors=None):\n",
        "\n",
        "  if special_tokens is not None:\n",
        "    text_dat = text_dat + [special_tokens * min_freq]\n",
        "\n",
        "  if isinstance(learned_vectors, Vectors):\n",
        "    Field.build_vocab(text_dat, min_freq=min_freq, vectors=learned_vectors)\n",
        "  else:\n",
        "    Field.build_vocab(text_dat, min_freq=min_freq)\n",
        "  \n",
        "  return Field\n"
      ],
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nTFvOqdK0ER",
        "outputId": "4cf50fd4-b995-4e59-d415-be80fd92b526"
      },
      "source": [
        "TEXT = build_Field_with_vocab(text_dat=tr_docs, Field=TEXT, special_tokens =['<cls>', '<sep>'], min_freq=10, learned_vectors=glove_6b_300d_vec)"
      ],
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6211\n",
            "[['=', '=', 'control', '=', '=', '=', '<eos>', '<eos>', 'because', 'of', 'the', 'damage', 'they', 'do', ',', 'there', 'have', 'been', 'attempts', 'to', 'control', 'the', 'numbers', 'of', 'both', 'native', 'and', 'introduced', 'populations', 'of', 'common', 'starlings', '.', 'within', 'the', 'natural', 'breeding', 'range', ',', 'this', 'may', 'be', 'affected', 'by', 'legislation', '.', 'for', 'example', ',', 'in', 'spain', ',', 'this', 'is', 'a', 'species', 'hunted', 'commercially', 'as', 'a', 'food', 'item', ',', 'and', 'has', 'a', 'close', 'season', ',', 'whereas', 'in', 'france', ',', 'it', 'is', 'classed', 'as', 'a', 'pest', ',', 'and', 'the', 'season', 'in', 'which', 'it', 'may', 'be', 'killed', 'covers', 'the', 'greater', 'part', 'of', 'the', 'year', '.', 'in', 'the', 'uk', ',', 'the', 'common', 'starling', 'may', 'be', 'killed', 'at', 'any', 'time', 'of', 'year', '.', 'this', 'species', 'is', 'migratory', ',', 'so', 'birds', 'involved', 'in', 'control', 'measures', 'may', 'have', 'come', 'from', 'a', 'wide', 'area', 'and', 'breeding', 'populations', 'may', 'not', 'be', 'greatly', 'affected', '.', 'in', 'europe', ',', 'the', 'varying', 'legislation', 'and', 'mobile', 'populations', 'mean', 'that', 'control', 'attempts', 'may', 'have', 'limited', 'long', '@ - @', 'term', 'results', '.', 'non', '@ - @', 'lethal', 'techniques', 'such', 'as', '< unk >', 'with', 'visual', 'or', 'auditory', 'devices', 'have', 'only', 'a', 'temporary', 'effect', 'in', 'any', 'case', '.', '<eos>', 'huge', 'urban', 'roosts', 'in', 'cities', 'can', 'create', 'problems', 'due', 'to', 'the', 'noise', 'and', 'mess', 'made', 'and', 'the', 'smell', 'of', 'the', 'droppings', '.', 'in', '1949', ',', 'so', 'many', 'birds', 'landed', 'on', 'the', 'clock', 'hands', 'of', 'london', \"' s\", 'big', 'ben', 'that', 'it', 'stopped', ',', 'leading', 'to', 'unsuccessful', 'attempts', 'to', 'disrupt', 'the', 'roosts', 'with', 'netting', ',', 'repellent', 'chemical', 'on', 'the', 'ledges', 'and', 'broadcasts', 'of', 'common', 'starling', 'alarm', 'calls', '.', 'an', 'entire', 'episode', 'of', 'the', '< unk >', 'show', 'in', '1954', 'was', 'a', 'parody', 'of', 'the', 'futile', 'efforts', 'to', 'disrupt', 'the', 'large', 'common', 'starling', 'roosts', 'in', 'central', 'london', '.', '<eos>', 'where', 'it', 'is', 'introduced', ',', 'the', 'common', 'starling', 'is', 'unprotected', 'by', 'legislation', ',', 'and', 'extensive', 'control', 'plans', 'may', 'be', 'initiated', '.', 'common', 'starlings', 'can', 'be', 'prevented', 'from', 'using', 'nest', 'boxes', 'by', 'ensuring', 'that', 'the', 'access', 'holes', 'are', 'smaller', 'than', 'the', '1', '@ . @', '5', 'in', '(', '38', 'mm', ')', 'diameter', 'they', 'need', ',', 'and', 'the', 'removal', 'of', '< unk >', '< unk >', 'them', 'from', 'visiting', 'bird', '< unk >', '.', '<eos>', 'western', 'australia', 'banned', 'the', 'import', 'of', 'common', 'starlings', 'in', '1895', '.', 'new', 'flocks', 'arriving', 'from', 'the', 'east', 'are', 'routinely', 'shot', ',', 'while', 'the', 'less', 'cautious', 'juveniles', 'are', 'trapped', 'and', 'netted', '.', 'new', 'methods', 'are', 'being', 'developed', ',', 'such', 'as', '< unk >', 'one', 'bird', 'and', 'tracking', 'it', 'back', 'to', 'establish', 'where', 'other', 'members', 'of', 'the', 'flock', 'roost', '.', 'another', 'technique', 'is', 'to', '< unk >', 'the', 'dna', 'of', 'australian', 'common', 'starling', 'populations', 'to', 'track', 'where', 'the', 'migration', 'from', 'eastern', 'to', 'western', 'australia', 'is', 'occurring', 'so', 'that', 'better', '< unk >', 'strategies', 'can', 'be', 'used', '.', 'by', '2009', ',', 'only', '300', 'common', 'starlings', 'were', 'left', 'in', 'western', 'australia', ',', 'and', 'the', 'state', 'committed', 'a', 'further', 'a', '$', '400', '@ , @', '000', 'in', 'that', 'year', 'to', 'continue', 'the', 'eradication', 'programme', '.', '<eos>', 'in', 'the', 'united', 'states', ',', 'common', 'starlings', 'are', 'exempt', 'from', 'the', '< unk >', 'bird', 'treaty', 'act', ',', 'which', 'prohibits', 'the', 'taking', 'or', 'killing', 'of', 'migratory', 'birds', '.', 'no', 'permit', 'is', 'required', 'to', 'remove', 'nests', 'and', 'eggs', 'or', 'kill', 'juveniles', 'or', 'adults', '.', 'research', 'was', 'undertaken', 'in', '1966', 'to', 'identify', 'a', 'suitable', '< unk >', 'that', 'would', 'both', 'kill', 'common', 'starlings', 'and', 'would', 'readily', 'be', 'eaten', 'by', 'them', '.', 'it', 'also', 'needed', 'to', 'be', 'of', 'low', 'toxicity', 'to', 'mammals', 'and', 'not', 'likely', 'to', 'cause', 'the', 'death', 'of', 'pets', 'that', 'ate', 'dead', 'birds', '.', 'the', 'chemical', 'that', 'best', 'fitted', 'these', 'criteria', 'was', 'drc', '@ - @', '< unk >', ',', 'now', 'marketed', 'as', '< unk >', '.', 'in', '2008', ',', 'the', 'united', 'states', 'government', 'poisoned', ',', 'shot', 'or', 'trapped', '1', '@ . @', '7', 'million', 'birds', ',', 'the', 'largest', 'number', 'of', 'any', 'nuisance', 'species', 'to', 'be', 'destroyed', '.', 'in', '2005', ',', 'the', 'population', 'in', 'the', 'united', 'states', 'was', 'estimated', 'at', '140', 'million', 'birds', ',', 'around', '45', '%', 'of', 'the', 'global', 'total', 'of', '310', 'million', '.', '<eos>'], ['=', '=', 'in', 'science', 'and', 'culture', '=', '=', '=', '<eos>', '<eos>', 'common', 'starlings', 'may', 'be', 'kept', 'as', 'pets', 'or', 'as', 'laboratory', 'animals', '.', 'austrian', '< unk >', 'konrad', 'lorenz', 'wrote', 'of', 'them', 'in', 'his', 'book', 'king', 'solomon', \"' s\", 'ring', 'as', '\"', 'the', 'poor', 'man', \"' s\", 'dog', '\"', 'and', '\"', 'something', 'to', 'love', '\"', ',', 'because', 'nestlings', 'are', 'easily', 'obtained', 'from', 'the', 'wild', 'and', 'after', 'careful', 'hand', 'rearing', 'they', 'are', 'straightforward', 'to', 'look', 'after', '.', 'they', 'adapt', 'well', 'to', 'captivity', ',', 'and', 'thrive', 'on', 'a', 'diet', 'of', 'standard', 'bird', 'feed', 'and', '< unk >', '.', 'several', 'birds', 'may', 'be', 'kept', 'in', 'the', 'same', 'cage', ',', 'and', 'their', '< unk >', 'makes', 'them', 'easy', 'to', 'train', 'or', 'study', '.', 'the', 'only', 'disadvantages', 'are', 'their', '< unk >', 'and', 'indiscriminate', 'defecation', 'habits', 'and', 'the', 'need', 'to', 'take', 'precautions', 'against', 'diseases', 'that', 'may', 'be', 'transmitted', 'to', 'humans', '.', 'as', 'a', 'laboratory', 'bird', ',', 'the', 'common', 'starling', 'is', 'second', 'in', 'numbers', 'only', 'to', 'the', 'domestic', '< unk >', '.', '<eos>', 'the', 'common', 'starling', \"' s\", 'gift', 'for', 'mimicry', 'has', 'long', 'been', 'recognised', '.', 'in', 'the', 'medieval', 'welsh', '< unk >', ',', '< unk >', '< unk >', 'a', 'common', 'starling', ',', '\"', 'taught', 'it', 'words', '\"', ',', 'and', 'sent', 'it', 'across', 'the', 'irish', 'sea', 'with', 'a', 'message', 'to', 'her', 'brothers', ',', '< unk >', 'and', '< unk >', ',', 'who', 'then', 'sailed', 'from', 'wales', 'to', 'ireland', 'to', 'rescue', 'her', '.', 'pliny', 'the', 'elder', 'claimed', 'that', 'these', 'birds', 'could', 'be', 'taught', 'to', 'speak', 'whole', 'sentences', 'in', 'latin', 'and', 'greek', ',', 'and', 'in', 'henry', 'iv', ',', 'william', 'shakespeare', 'had', 'hotspur', 'declare', '\"', 'the', 'king', 'forbade', 'my', 'tongue', 'to', 'speak', 'of', 'mortimer', '.', 'but', 'i', 'will', 'find', 'him', 'when', 'he', 'is', 'asleep', ',', 'and', 'in', 'his', 'ear', 'i', \"' ll\", '< unk >', \"'\", 'mortimer', '!', \"'\", '< unk >', 'i', \"' ll\", 'have', 'a', 'starling', 'shall', 'be', 'taught', 'to', 'speak', 'nothing', 'but', 'mortimer', ',', 'and', 'give', 'it', 'to', 'him', 'to', 'keep', 'his', 'anger', 'still', 'in', 'motion', '.', '\"', '<eos>', 'mozart', 'had', 'a', 'pet', 'common', 'starling', 'which', 'could', 'sing', 'part', 'of', 'his', 'piano', 'concerto', 'in', 'g', 'major', '(', '< unk >', '.', '453', ')', '.', 'he', 'had', 'bought', 'it', 'from', 'a', 'shop', 'after', 'hearing', 'it', 'sing', 'a', 'phrase', 'from', 'a', 'work', 'he', 'wrote', 'six', 'weeks', 'previously', ',', 'which', 'had', 'not', 'yet', 'been', 'performed', 'in', 'public', '.', 'he', 'became', 'very', 'attached', 'to', 'the', 'bird', 'and', 'arranged', 'an', 'elaborate', 'funeral', 'for', 'it', 'when', 'it', 'died', 'three', 'years', 'later', '.', 'it', 'has', 'been', 'suggested', 'that', 'his', 'a', 'musical', '< unk >', '(', 'k .', '522', ')', 'might', 'be', 'written', 'in', 'the', 'comical', ',', '< unk >', 'style', 'of', 'a', 'starling', \"' s\", '< unk >', '.', 'other', 'people', 'who', 'have', 'owned', 'common', 'starlings', 'report', 'how', 'adept', 'they', 'are', 'at', 'picking', 'up', 'phrases', 'and', 'expressions', '.', 'the', 'words', 'have', 'no', 'meaning', 'for', 'the', 'starling', ',', 'so', 'they', 'often', 'mix', 'them', 'up', 'or', 'use', 'them', 'on', 'what', 'to', 'humans', 'are', 'inappropriate', 'occasions', 'in', 'their', 'songs', '.', 'their', 'ability', 'at', 'mimicry', 'is', 'so', 'great', 'that', 'strangers', 'have', 'looked', 'in', 'vain', 'for', 'the', 'human', 'they', 'think', 'they', 'have', 'just', 'heard', 'speak', '.', '<eos>', 'common', 'starlings', 'are', 'trapped', 'for', 'food', 'in', 'some', 'mediterranean', 'countries', '.', 'the', 'meat', 'is', 'tough', 'and', 'of', 'low', 'quality', ',', 'so', 'it', 'is', '< unk >', 'or', 'made', 'into', '< unk >', '.', 'one', 'recipe', 'said', 'it', 'should', 'be', '< unk >', '\"', 'until', 'tender', ',', 'however', 'long', 'that', 'may', 'be', '\"', '.', 'even', 'when', 'correctly', 'prepared', ',', 'it', 'may', 'still', 'be', 'seen', 'as', 'an', 'acquired', 'taste', '.', '<eos>', '<eos>'], ['<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>', '<cls>', '<sep>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQpntrmr2rn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "5b2b5ec7-e0f6-47bb-8b76-89378d1c3a45"
      },
      "source": [
        "class Xlnet_Input(torch.utils.data.Dataset):\n",
        "  \"\"\"Creates input data with unlabeled text data\n",
        "   input : list of unlabeled text in list form\n",
        "   output : dataset, list of tuple(Text, seq_len, context, relative_position, segment)\"\"\"\n",
        "  def __init__(self, text_dat: list, Field: Field, sep_docs: bool=True, doc_sep: str='<eos>  =', token_sep : str='  '):\n",
        "    \"\"\"text_dat: list of unlabeled documents(list of lists), should be tokenized with 'revtok'\n",
        "    \"\"\"\n",
        "    #dict\n",
        "    #context_encoding, segment_encoding\n",
        "    #positional encoding\n",
        "\n",
        "    self.doc_sep = doc_sep\n",
        "    self.token_sep = token_sep\n",
        "\n",
        "    if sep_docs:\n",
        "      text_dat = self.sep_to_docs(text_dat)\n",
        "    \n",
        "    if special_tokens is not None:\n",
        "      text_dat.append(special_tokens)\n",
        "\n",
        "    if build_dict:\n",
        "      TEXT.build_vocab(tr_docs, min_freq=10, vectors=)\n",
        "      \n",
        "\n",
        "    \n",
        "    self.text_dat =[]\n",
        "    for doc in text_dat:\n",
        "      self.text_dat.append(self.Field.numericalize([doc]))\n",
        "    \n",
        "    self.text_dat = []\n",
        "    self.seq_len = []\n",
        "    self.context = []\n",
        "    self.relative_position = []\n",
        "    self.segment = []\n",
        "    self.input = []\n",
        "\n",
        "  def create_xlnet_input(self, segment_length: int, reuse_length: int, seq_length: int):\n",
        "    \"\"\"creates input data sample for xlnet,\n",
        "    create all dataset and allocate memory for those dataset.\n",
        "    this will reduce time for __getitem__ operation\"\"\"\n",
        "    #self.segment_len = segment_len\n",
        "    self.seq_length = seq_length\n",
        "    self.reuse_length = reuse_length\n",
        "\n",
        "\n",
        "    self.segmentation(self, segment_len)\n",
        "\n",
        "\n",
        "    self.text_segment = []\n",
        "    self.position_segment = []\n",
        "    self.segment_index = []\n",
        "    self.doc_index = []\n",
        "    \n",
        "    self.input = []\n",
        "\n",
        "    del self.input_text\n",
        "    del self.text_len\n",
        "    del self.context\n",
        "    del self.relative_position\n",
        "    del self.segment\n",
        "    \n",
        "  def segmentation(self, segment_len):\n",
        "    \"\"\"split input text into segments\n",
        "    inputs \n",
        "      segment_len : length to which input text is splitted\n",
        "    \n",
        "    creates\n",
        "      text_segment : segmented text data\n",
        "      position_segment : positional information of text segments\n",
        "      segment_index : index of segment\n",
        "      doc_index : index of the documnet(or context)\n",
        "    \"\"\"\n",
        "    self.text_segment = []\n",
        "    self.position_segment = []\n",
        "    self.segment_index = []\n",
        "    self.doc_index = []\n",
        "    \n",
        "    curr_doc_index = 0\n",
        "\n",
        "    if()\n",
        "\n",
        "    for curr_doc in self.input_text:\n",
        "\n",
        "      self.segment_index.append(list(range(len(curr_doc) // self.segment_len + 1)))\n",
        "\n",
        "      self.doc_index.append([curr_doc_index] * len(self.segment_index))\n",
        "\n",
        "      curr_doc_index += 1\n",
        "\n",
        "      for curr_index in self.segment_index[-1]:\n",
        "        \n",
        "        self.text_segment.append(self.text_dat[self.segment_len * curr_index: (self.segment_len * curr_index + 1)])\n",
        "        \n",
        "        last_segment_len = len(text_segment[-1])\n",
        "        \n",
        "        self.position_segment.append( list( range( self.segment_len * curr_index, self.segment_len * curr_index + last_segment_len)))\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "  def sep_to_docs(self) -> list:\n",
        "  #def sep_to_docs(data : list, doc_sep : str, token_sep : str) -> list:\n",
        "  \"\"\"returns documents from split input text chunk\n",
        "  \"\"\"\n",
        "    result = []\n",
        "    docs = (''.join(data)).split(doc_sep)\n",
        "    for i in docs:\n",
        "      curr_doc = (''.join(i)).split(token_sep)\n",
        "      curr_doc = [i for i in curr_doc if i != '']\n",
        "      if len(curr_doc) >= 1:\n",
        "        result.append(curr_doc)\n",
        "    return result\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    doc_paths = os.path.join(self.root_dir, self.target_csv.iloc[idx, 0])\n",
        "    for doc_path in doc_paths\n",
        "    doc_file = open(doc_path, 'r')\n",
        "\n",
        "    lines = doc_file.readlines()\n",
        "\n",
        "    if word_dict:\n",
        "      for line in lines:\n",
        "        for \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    doc_file.close()\n",
        "\n",
        "\n",
        "    f = open(\"C:/doit/새파일.txt\", 'r')\n",
        "\n",
        "\n",
        "      landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "      landmarks = np.array([landmarks])\n",
        "      landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "      sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-291-a83c5f36965b>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    def __init__(self, text_dat: list of docs, Field: Field, sep_docs: bool=True, special_tokens: list=None, doc_sep : str='<eos>  =', token_sep : str='  '):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM1qrCdOq8NC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f99ac96a-eee5-4b37-d816-8c4290d169db"
      },
      "source": [
        "self.target_csv = pd.read_csv(traget_csv)\n",
        "    self.dat_shape = data.shape\n",
        "    self.root_dir = root_dir\n",
        "    self.segment_len = segment_len\n",
        "    self.word_dict = False\n",
        "    self.data = data\n",
        "\n",
        "    if do_shuffle:\n",
        "      shuffle()\n",
        "    \n",
        "    segmentation()\n",
        "\n",
        "  def segmentation(self):\n",
        "    self.segments = \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    doc_paths = os.path.join(self.root_dir, self.target_csv.iloc[idx, 0])\n",
        "    for doc_path in doc_paths\n",
        "    doc_file = open(doc_path, 'r')\n",
        "\n",
        "    lines = doc_file.readlines()\n",
        "\n",
        "    if word_dict:\n",
        "      for line in lines:\n",
        "        for \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    doc_file.close()\n",
        "\n",
        "\n",
        "    f = open(\"C:/doit/새파일.txt\", 'r')\n",
        "\n",
        "\n",
        "      landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "      landmarks = np.array([landmarks])\n",
        "      landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "      sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<eos>  ='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB1W23lzrcgC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv5OFVg2XxFH"
      },
      "source": [
        "aa = [tr_dat.examples[0].text][0][:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwJmxL-iare"
      },
      "source": [
        "result = []\n",
        "docs = (''.join(aa)).split('<eos>  =')\n",
        "for i in temp:\n",
        "  curr_doc = (''.join(i)).split('  ')\n",
        "  curr_doc = [i for i in curr_doc if i != '']\n",
        "  result.append(curr_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMDNrB8iYRjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3656384b-d915-4a69-d3f1-f7d9f982dbe4"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,    29,  4031,  2997,   447,  3285,     4,   133,   444,    36,\n",
              "         3745,     3,   838,     6, 24073,  1484,     4,    25, 13603,    71])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKr0gTe1ZL5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0421253f-8def-4ee5-bcac-c2a75ceefc04"
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,  1781,    20,   179, 15130,   765,   632,    59,   490,  6997,\n",
              "           27,     7,     6,     4,   480,  5051,    60,    15,     8,     9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvk-s2rAYE3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d55e501-8c7b-4ff5-a964-c6664ea3c13b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Field.build_vocab of <torchtext.data.field.Field object at 0x7f1ca121d400>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt9bcA38QVly"
      },
      "source": [
        "en_textfield.build_vocab(tr_dat, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usTUPO0SSHH0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RRFhrAJuhLT"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (tr_dat, val_dat, test_dat),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65YpZ6lRctRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35f9ac9-d473-41b0-a6d6-8a19fd8ebd2d"
      },
      "source": [
        "bb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   9,   11, 3875,  ...,    4,    9,    9]]), tensor([2088628]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjRwKvDVZE6"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvAhDzXVo4x"
      },
      "source": [
        "ds = tfds.load('glue', split='train', as_supervised=False, shuffle_files=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGywEswRWqYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d6a25df3-bf9a-46fd-bbf6-4935f1c62d69"
      },
      "source": [
        "for i in ds:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cc2ab6e2e158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1LN41CIVcxW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "9891165d-ad56-4226-983b-94fd97eea533"
      },
      "source": [
        "aa.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-603b00bbb490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Glue' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA54tjsGB73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "615a630b-f343-43e0-ef74-bdd72e2cf185"
      },
      "source": [
        "np.array([1,2,3]).shape()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a69473d3b56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFIb0Ta-9pt"
      },
      "source": [
        "ddd = aa(a = np.array([1,2,4,5]),b=np.array([1,2,4,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZZQirEK_0-H"
      },
      "source": [
        "exec('b=ddd.a.sum()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "norvzpPi_0Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "94505df7-9556-490d-91cc-cee5ed60ee07"
      },
      "source": [
        "len(np.array([1,2,3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8neGAK_MSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "59268c2b-827c-4152-aa90-5485f0d97107"
      },
      "source": [
        "sample_dict.get('aa')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qisDhWYI_Agt"
      },
      "source": [
        "sample_dict = {'aa': 12, 'bb':13}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3gvdyAs-6mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "176553f5-8ace-4c86-abde-e8c0fea3275a"
      },
      "source": [
        "'list is list'.split(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'is', 'list']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghK98IUDdOeV"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c2UthJ4dXb7"
      },
      "source": [
        "#활용된 잡지식\n",
        "\n",
        "#pytorch dataset 불러오기 \n",
        "#https://pytorch.org/text/datasets.html\n",
        "\n",
        "#자동으로 추가해야할 내용들\n",
        "#To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference\n",
        "\n",
        "#데이터 로딩\n",
        "#data loading\n",
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
        "\n",
        "#함수 오버라이딩 방법\n",
        "#super(MyModel, self).__init__()\n",
        "\n",
        "#einstein summation\n",
        "#https://rockt.github.io/2018/04/30/einsum\n",
        "\n",
        "#defining method with string\n",
        "#https://stackoverflow.com/questions/11553721/using-a-string-variable-as-a-variable-name\n",
        "\n",
        "#kwargs,args\n",
        "#https://brunch.co.kr/@princox/180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLLgpI56dZnK"
      },
      "source": [
        "# Two-stream attention cell\n",
        "#input : q_size, k_size, v_size, segment_index, attention_size, heads =8, **kwargs = {init_Qstream_q_LT_weight, init_Qstream_q_K_weight,init_Qstream_q_R_weight, init_Qstream_k_E_LT_weight, init_Qstream_k_R_LT_weight, init_Qstream_v_LT_weight,\n",
        "#                                                                            init_Qstream_segment_same_weight, init_Qstream_segment_notsame_weight, init_Qstream_segment_bias, init_Qstream_output_weight,\n",
        "#                                                                            init_Cstream_q_LT_weight, init_Cstream_q_K_weight,init_Cstream_q_R_weight, init_Cstream_k_E_LT_weight, init_Cstream_k_R_LT_weight, init_Cstream_v_LT_weight,\n",
        "#                                                                            init_Cstream_segment_same_weight, init_Cstream_segment_notsame_weight, init_Cstream_segment_bias, init_Cstream_output_weight}\n",
        "#\n",
        "\n",
        "#operation : q_LT, k_LT, v_LT, segmentation, permutation(optional), masking, relative_position_encoding, segment_encoding, cal_content_attention, cal_query_attention, memorise\n",
        "\n",
        "#properties : Qstream_q_LT_weight, Qstream_q_K_weight,Qstream_q_R_weight, Qstream_k_E_LT_weight, Qstream_k_R_LT_weight, Qstream_v_LT_weight,\n",
        "#             Qstream_segment_same_weight, Qstream_segment_notsame_weight, Qstream_segment_bias, Qstream_output_weight,\n",
        "#             Cstream_q_LT_weight, Cstream_q_K_weight,Cstream_q_R_weight, Cstream_k_E_LT_weight, Cstream_k_R_LT_weight, Cstream_v_LT_weight,\n",
        "#             Cstream_segment_same_weight, Cstream_segment_notsame_weight, Cstream_segment_bias, Cstream_output_weight content_attention, query_attention, content_memory, query_memory\n",
        "\n",
        "#output : Dq vector(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9SPAirbqb1a"
      },
      "source": [
        "class create_PLM_input(data_utils.Dataset):\n",
        "  def __init__(self, data, target_csv, word_dict = False, sep_by_lines = False, segment_len = False, do_shuffle=True):\n",
        "    super(create_PLM_input, self).__init__()\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4950byzWJBsJ"
      },
      "source": [
        "class create_PLM_input_inmemory(data_utils.Dataset):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edo49bgpmrCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e768e5b7-29bf-443e-b8cc-16209fbd144f"
      },
      "source": [
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perred\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V_hdr4sDpfz"
      },
      "source": [
        "a= torch.nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS8PYlKKEmbS"
      },
      "source": [
        "def shuffle()\n",
        "\n",
        "def permutate()\n",
        "\n",
        "def rel_position_encoding():\n",
        "\n",
        "def masking():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Fa0rBOEpbf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQmmsxUrmp8q"
      },
      "source": [
        "if not a:\n",
        "  print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHFApw4UPpar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "6a74a673-3317-4367-fcb9-72efeaa73f9a"
      },
      "source": [
        "class Two_stream_attention(nn):\n",
        "  def __init__(self, q_size, k_size, v_size, attention_size = False, heads =8, **kwargs):\n",
        "    super(Two_stream_attention, self).__init__() \n",
        "      \n",
        "\n",
        "    self.segment_index = segment_index\n",
        "\n",
        "    #initialize params\n",
        "    self.initialize_params(self, q_size, k_size, v_size, attention_size, heads)\n",
        "\n",
        "  def initialize_params(self, q_size, k_size, v_size, attention_size, heads):\n",
        "    self.Qstream_q_LT_weight = nn.Linear()\n",
        "    self.Qstream_q_K_weight = \n",
        "    self.Qstream_q_R_weight = \n",
        "    \n",
        "    self.Qstream_k_E_LT_weight = \n",
        "    self.Qstream_k_R_LT_weight = \n",
        "    self.Qstream_v_LT_weight = \n",
        "    \n",
        "    self.Qstream_segment_same_weight = \n",
        "    self.Qstream_segment_notsame_weight = \n",
        "    self.Qstream_segment_bias = \n",
        "    \n",
        "    self.Qstream_output_weight = \n",
        "\n",
        "    self.Cstream_q_LT_weight = \n",
        "    self.Cstream_q_K_weight = \n",
        "    self.Cstream_q_R_weight = \n",
        "    \n",
        "    self.Cstream_k_E_LT_weight = \n",
        "    self.Cstream_k_R_LT_weight = \n",
        "    self.Cstream_v_LT_weight = \n",
        "\n",
        "    self.Cstream_segment_same_weight = \n",
        "    self.Cstream_segment_notsame_weight = \n",
        "    self.Cstream_segment_bias = \n",
        "    \n",
        "    self.Cstream_output_weight =\n",
        "\n",
        "  def set_params(self,**kwargs):\n",
        "    for i,j in kwargs.items():\n",
        "      setattr(self, i, j)\n",
        "      explicitly_defined_vars.append(i)\n",
        "\n",
        "  def forward(self, input, do_parm = False, rel_position = False):\n",
        "    \n",
        "    #record position\n",
        "    if not rel_position:\n",
        "      self.rel_position = rel_position_encoding(input)\n",
        "    else:\n",
        "      self.rel_position = rel_position\n",
        "\n",
        "    #permutation\n",
        "    if do_parm:\n",
        "      self.permutate()\n",
        "\n",
        "    #get position encoding\n",
        "\n",
        "    #cal\n",
        "  def backward(self,??): #클레스 정의하는 법 참조\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ad5720c06428>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    self.Qstream_q_K_weight =\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laXkR4scazxZ"
      },
      "source": [
        "_#http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxPfane-X3-A"
      },
      "source": [
        "##Copyright 2013, Youngbin Jang, All rights reserved."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grV-X_-sfTcW"
      },
      "source": [
        "#https://tutorials.pytorch.kr/beginner/nn_tutorial.html"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}