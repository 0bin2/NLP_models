{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XL_net",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNerPvxalnnoQPUL3/5c7zy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingbingodbin/NLP_models/blob/main/XLnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBg14N-WRqy"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data import Field\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from itertools import combinations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTeEn1OvuFKG"
      },
      "source": [
        "import torchtext\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXsNFpb_2nUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31f5ede-8a80-4029-f03f-3ee6c1490896"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36z8vshT7jVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7875c7be-f97f-4de0-90d7-0eb49884790b"
      },
      "source": [
        "import os\n",
        "os.listdir('drive/My Drive/datasets/wikitext-2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wiki.valid.tokens', 'wiki.test.tokens', 'wiki.train.tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXa80D3W5vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27485363-26f4-46ac-d154-432af9fa9fd4"
      },
      "source": [
        "#install package for subword tokenizer\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To7FLwGYVLvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d83962d-6f8f-47ee-92d3-1332eb599215"
      },
      "source": [
        "#install package for subword tokenizer\n",
        "!pip install revtok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZiU1OptMLU"
      },
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2PocX6mWuZn"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xItrfu9Tc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9b62cc-6bde-420d-d7a2-29afb753bc8c"
      },
      "source": [
        "import urllib.request\n",
        "import io\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Loads model from URL as iterator and stores the model to BytesIO.\n",
        "model = io.BytesIO()\n",
        "with urllib.request.urlopen(\n",
        "    'https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt'\n",
        ") as response:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      sentence_iterator=response, model_writer=model, vocab_size=1000)\n",
        "\n",
        "# Serialize the model as file.\n",
        "# with open('out.model', 'wb') as f:\n",
        "#   f.write(model.getvalue())\n",
        "\n",
        "# Directly load the model from serialized model.\n",
        "sp = spm.SentencePieceProcessor(model_proto=model.getvalue())\n",
        "print(sp.encode('this is test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[64, 47, 4, 13, 391]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GskmwEYG97uF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b8f1a1b-6465-491a-e4d4-49ed9abe78ce"
      },
      "source": [
        "\"\"\"load wikitext2 data\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'load wikitext2 data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxS-wshN0_VC"
      },
      "source": [
        "tokenizer = get_tokenizer('revtok')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chaubPwz2vjL"
      },
      "source": [
        "dataset_base_path = 'drive/My Drive/datasets/'\n",
        "path = dataset_base_path + 'wikitext-2/'\n",
        "#use reversable tokenizer for flexible \n",
        "TEXT = torchtext.data.ReversibleField(tokenize = get_tokenizer(\"revtok\"),\n",
        "                            init_token = '<sos>',\n",
        "                            eos_token = '<eos>',\n",
        "                            unk_token ='<unk>',\n",
        "                            sequential=True,\n",
        "                            lower=True,\n",
        "                            use_vocab=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntnbNtxN3zF"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmeKUbkw84SV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5ad245-4318-453c-b33e-1fee5d6c0612"
      },
      "source": [
        "tr_dat,val_dat,te_dat = torchtext.datasets.language_modeling.WikiText2.splits(TEXT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 8.46MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSkmoEkSUeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f118d0-46f1-455a-bcdf-c1347d6baa72"
      },
      "source": [
        "[tr_dat.examples[0].text][0][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " ' ',\n",
              " '<eos>',\n",
              " ' ',\n",
              " ' = ',\n",
              " ' valkyria ',\n",
              " ' chronicles ',\n",
              " ' iii ',\n",
              " ' = ',\n",
              " ' ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n6QXjlrVet_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73c26fd-35dd-40c3-d3bb-63fe72954821"
      },
      "source": [
        "[val_dat.examples[0].text][0][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ', '<eos>', ' ', ' = ', ' homarus ', ' gammarus ', ' = ', ' ', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01iBnn8Vh5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9525a62c-df30-4060-8d25-661c47dfaefe"
      },
      "source": [
        "[te_dat.examples[0].text][0][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', ' ', '<eos>', ' ', ' = ', ' robert ', ' <', ' unk ', '> ', ' = ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSlLqon-6F0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fce9d3-d265-48ed-8cdc-d75363e1b4cd"
      },
      "source": [
        "hasattr(TEXT, \"vocab\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFqSs_Ftg7Fd"
      },
      "source": [
        "#seperating wikitext2 data to documents\n",
        "def sep_to_docs(data : list, doc_sep : str, token_sep : str) -> list:\n",
        "  \"\"\"returns documents from split input text chunk\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  docs = (''.join(data)).split(doc_sep)\n",
        "  for i in docs:\n",
        "    curr_doc = (''.join(i)).split(token_sep)\n",
        "    curr_doc = [i for i in curr_doc if i != '']\n",
        "    if len(curr_doc) >= 1:\n",
        "      result.append(curr_doc)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rilTTz4oroyR"
      },
      "source": [
        "tr_docs = sep_to_docs(data = [tr_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')\n",
        "val_docs = sep_to_docs(data = [val_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')\n",
        "te_docs = sep_to_docs(data = [te_dat.examples[0].text][0], doc_sep = '<eos>  =', token_sep = '  ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Kl4dHcsH34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d494a0-7280-46fd-c6b5-e91266cd4f40"
      },
      "source": [
        "print(len(tr_docs),len(val_docs),len(te_docs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6211 620 708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82N7GhvPWqho"
      },
      "source": [
        "#create vocab with glove.6B.100d\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10,vectors=\"glove.6B.100d\")\n",
        "#!cp .vector_cache/glove.6B.zip drive/\"My Drive\"/datasets/\n",
        "\n",
        "\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qznBPpMilCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4444eccf-c908-4eb8-9e60-922f5bc4620f"
      },
      "source": [
        "\"\"\"extracting vocab\"\"\"\n",
        "#TEXT.build_vocab(tr_docs,min_freq=10,vectors=\"glove.6B.100d\")\n",
        "#!cp .vector_cache/glove.6B.zip drive/\"My Drive\"/datasets/\n",
        "#glove_zip = zipfile.ZipFile('drive/My Drive/datasets/glove.6B.zip')\n",
        "#glove_zip.extractall('drive/My Drive/datasets/pretrained_wordvectors')\n",
        "#glove_zip.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'extracting vocab'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze4U_d__iEdv"
      },
      "source": [
        "from torchtext.vocab import Vectors\n",
        "glove_6b_300d_vec = Vectors(name='glove.6B.300d.txt', cache='drive/My Drive/datasets/pretrained_wordvectors')\n",
        "\n",
        "special_tokens = '<cls>'\n",
        "TEXT.build_vocab(tr_docs, min_freq=10, vectors=glove_6b_300d_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1SufvaQb68"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNkbFZLcm-jz"
      },
      "source": [
        "vocab_size = len(TEXT.vocab.itos)\n",
        "word_embedding_size = TEXT.vocab.vectors.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV69mA7wXiQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "13c924d4-db93-443b-b69f-d31e569fbaa3"
      },
      "source": [
        "#sample embedding\n",
        "embeds = nn.Embedding(vocab_size, word_embedding_size)\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-853a7b9a375f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sample embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlookup_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhello_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhello_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_to_ix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpcZ9JPjgVK2"
      },
      "source": [
        "embeds = nn.Embedding(vocab_size, word_embedding_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJUz8Uthzdar"
      },
      "source": [
        "aa = TEXT.numericalize([te_docs[0][:10]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG_maAvz1uUh",
        "outputId": "a9fafe9d-fa64-4a2a-f815-b4468589cf4f"
      },
      "source": [
        "aa.squeeze(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([637,   8,  14,   3,   3, 637,   8,  26,  33, 340])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12tPV8-aiXWS",
        "outputId": "9b6c80e7-e2b2-4e65-83dc-e56d3de39c8f"
      },
      "source": [
        "TEXT.vocab.vectors[[TEXT.vocab.stoi.get('<sep>')]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mVC_u1I0wW"
      },
      "source": [
        "def build_Field_with_vocab(\n",
        "    text_dat: list,\n",
        "    Field: torchtext.data.Field,\n",
        "    special_tokens: list=None, \n",
        "    min_freq: int=10, \n",
        "    learned_vectors: Vectors=None):\n",
        "\n",
        "  if special_tokens is not None:\n",
        "    text_dat = text_dat + [special_tokens * min_freq]\n",
        "\n",
        "  if isinstance(learned_vectors, Vectors):\n",
        "    Field.build_vocab(\n",
        "        text_dat,\n",
        "        min_freq=min_freq,\n",
        "        vectors=learned_vectors\n",
        "        )\n",
        "  else:\n",
        "    Field.build_vocab(text_dat, min_freq=min_freq)\n",
        "  \n",
        "  return Field\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nTFvOqdK0ER"
      },
      "source": [
        "TEXT = build_Field_with_vocab(text_dat=tr_docs, Field=TEXT, special_tokens =['<cls>', '<sep>'], min_freq=10, learned_vectors=glove_6b_300d_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "j8eAkbuJbBF1",
        "outputId": "dc1ec008-77f0-46eb-9a4c-25ad6cead617"
      },
      "source": [
        "TEXT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ce5c5a06ada9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmQpntrmr2rn"
      },
      "source": [
        "class Xlnet_Input(torch.utils.data.Dataset):\n",
        "  \"\"\"Creates input data with unlabeled text data\n",
        "   input : list of unlabeled text in list form\n",
        "   output : dataset, list of tuple(Text, seq_len, context, relative_position, segment)\"\"\"\n",
        "  def __init__(self, text: list, Field: Field, do_perm: bool=True, sep_docs: bool=True, doc_sep: str='<eos>  =', token_sep : str='  '):\n",
        "    \"\"\"text_dat: list of unlabeled documents(list of lists), should be tokenized with 'revtok'\n",
        "    \"\"\"\n",
        "    #dict\n",
        "    #context_encoding, segment_encoding\n",
        "    #positional encoding\n",
        "\n",
        "    self.doc_sep = doc_sep\n",
        "    self.token_sep = token_sep\n",
        "    self.do_perm = do_perm\n",
        "    self.Field = Field\n",
        "\n",
        "    if sep_docs:\n",
        "      text = self.sep_to_docs(text, doc_sep, token_sep)\n",
        "\n",
        "    \n",
        "    self.text_dat =[]\n",
        "    for doc in text:\n",
        "      self.text_dat.append(self.Field.numericalize([doc]))\n",
        "  \n",
        "\n",
        "  def create_xlnet_input(self,segment_len: int, reuse_len: int, seq_len: int, d_model: int=256, fixed_length_segmentation: bool=True, random_segment_ratio: float=0.5, min_pred_len: int=5):\n",
        "\n",
        "    \"\"\"creates input data sample for xlnet,\n",
        "    create all dataset\n",
        "    __getitem__ \"\"\"\n",
        "\n",
        "    #self.segment_len = segment_len\n",
        "    self.d_model = d_model\n",
        "    self.min_pred_len = min_pred_len\n",
        "\n",
        "    if fixed_length_segmentation:\n",
        "      self.segmentation(segment_len, reuse_len)\n",
        "\n",
        "    sep_token = self.Field.vocab.stoi.get('<sep>')\n",
        "\n",
        "    cls_token = self.Field.vocab.stoi.get('<cls>')\n",
        "\n",
        "    #context, if doc\n",
        "\n",
        "    self.sampled_segment_pair = self.sample_segments(random_segment_ratio)\n",
        "\n",
        "    self.input = []\n",
        "\n",
        "    self.create_permed_masked_matrix(min_pred_len)\n",
        "\n",
        "    for first_idx,second_idx in self.sampled_segment_pair:\n",
        "        #self.permutation, masking을 둘다해줘야함\n",
        "      \n",
        "      self.input.append(\n",
        "          [\n",
        "            torch.cat(\n",
        "                (\n",
        "                    self.reuse_segment[first_idx],\n",
        "                    self.Field.vocab.vectors[[cls_token]],\n",
        "                    self.text_segment[first_idx], \n",
        "                    self.Field.vocab.vectors[[sep_token]],\n",
        "                    self.text_segment[second_idx],\n",
        "                    self.Field.vocab.vectors[[sep_token]]\n",
        "                ),\n",
        "            ),\n",
        "            torch.cat(\n",
        "                (\n",
        "                    self.Field.vocab.vectors[[cls_token]],\n",
        "                    self.target_text[first_idx], \n",
        "                    self.Field.vocab.vectors[[sep_token]],\n",
        "                    self.target_text[second_idx],\n",
        "                    self.Field.vocab.vectors[[sep_token]]\n",
        "                ),\n",
        "            ),\n",
        "\n",
        "            (\n",
        "                len(self.reuse_segment[first_idx]),\n",
        "                len(self.text_segment[first_idx]),\n",
        "                len(self.text_segment[second_idx])\n",
        "              ),\n",
        "            (\n",
        "                self.position_segment[first_idx],\n",
        "                self.position_segment[second_idx]\n",
        "              ),\n",
        "            (\n",
        "                first_idx,\n",
        "                second_idx\n",
        "              ),\n",
        "            (\n",
        "                self.doc_index[first_idx],\n",
        "                self.doc_index[second_idx]\n",
        "              ),\n",
        "            (\n",
        "                self.q_stream_perm_mask[first_idx]\n",
        "                self.c_stream_perm_mask[first_idx]\n",
        "                self.output_perm_mask[first_idx]\n",
        "            )\n",
        "            ]\n",
        "          )\n",
        "\n",
        "    \n",
        "    \n",
        "    self.input = tuple(self.input)\n",
        "    \n",
        "  def segmentation(self, segment_len, reuse_len):\n",
        "    \"\"\"split input text into segments\n",
        "    inputs \n",
        "      segment_len : length to which input text is splitted\n",
        "    \n",
        "    creates\n",
        "      text_segment : segmented text data\n",
        "      position_segment : positional information of text segments\n",
        "      segment_index : index of segment\n",
        "      doc_index : index of the documnet(or context)\n",
        "    \"\"\"\n",
        "    self.segment_len = segment_len\n",
        "    self.reuse_len = reuse_len\n",
        "\n",
        "    self.text_segment = []\n",
        "    self.target_text = []\n",
        "    self.reuse_segment = []\n",
        "    self.position_segment = []\n",
        "    self.segment_index = []\n",
        "    self.doc_index = []\n",
        "    \n",
        "    curr_doc_index = 0\n",
        "\n",
        "    for curr_doc in self.text_dat:\n",
        "\n",
        "      curr_segment_index = list(range(len(curr_doc) // self.segment_len + 1))\n",
        "      \n",
        "      self.segment_index += curr_segment_index\n",
        "\n",
        "      self.doc_index += [curr_doc_index] * len(curr_segment_index)\n",
        "\n",
        "      curr_doc_index += 1\n",
        "\n",
        "\n",
        "      for curr_index in curr_segment_index[:-1]:\n",
        "        \n",
        "        self.text_segment.append(\n",
        "             self.Field.vocab.vectors[\n",
        "                                curr_doc[\n",
        "                                              (self.segment_len * curr_index): (self.segment_len * (curr_index+1))\n",
        "                                              ]\n",
        "                                ].squeeze(1) \n",
        "            )\n",
        "        self.target_text.append(\n",
        "             self.Field.vocab.vectors[\n",
        "                                      curr_doc[\n",
        "                                               (self.segment_len * curr_index + 1): (self.segment_len * (curr_index+1) + 1)     \n",
        "                                      ]\n",
        "             ].squeeze(1)\n",
        "        )\n",
        "        self.reuse_segment.append(\n",
        "             self.Field.vocab.vectors[\n",
        "                                     curr_doc[\n",
        "                                                   ((self.segment_len * curr_index) - reuse_len):(self.segment_len * curr_index)\n",
        "                                                   ]\n",
        "                                ].squeeze(1)\n",
        "            )\n",
        "        \n",
        "        last_segment_len = len(self.text_segment[-1])\n",
        "        \n",
        "        self.position_segment.append(\n",
        "            torch.tensor(\n",
        "                range(\n",
        "                    self.segment_len * curr_index, self.segment_len * curr_index + last_segment_len\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "  def sample_segments(self, random_segment_ratio):\n",
        "    \"\"\"make segment pair for input data : [\"<cls>\",\"segmentA\",\"<sep>\",\"segmentB\",\"<sep>\"]\n",
        "    input:\n",
        "      random_segment_ratio : probability of choosing random segment instead of real next segment\n",
        "    output:\n",
        "      segment_pair: segment pair used in input data creation\n",
        "    \"\"\"\n",
        "    segment_pair = []\n",
        "    \n",
        "    n_segments = len(self.text_segment)\n",
        "\n",
        "    for i in range(n_segments-1):\n",
        "      if random_segment_ratio < np.random.uniform(size=1):\n",
        "        segment_pair.append(\n",
        "            (\n",
        "                i,\n",
        "                np.random.randint(0,n_segments-1),\n",
        "             )\n",
        "            )\n",
        "      else:\n",
        "        segment_pair.append(\n",
        "            (\n",
        "                i,i+1\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return tuple(segment_pair)\n",
        "    \n",
        "  def sep_to_docs(self, text, doc_sep, tokent_sep) -> list:\n",
        "  #def sep_to_docs(data : list, doc_sep : str, token_sep : str) -> list:\n",
        "    \"\"\"returns documents from split input text chunk\n",
        "    \"\"\"\n",
        "    seperated_docs = []\n",
        "    docs = (''.join(text)).split(doc_sep)\n",
        "    for i in docs:\n",
        "      curr_doc = (''.join(i)).split(token_sep)\n",
        "      curr_doc = [i for i in curr_doc if i != '']\n",
        "      if len(curr_doc) >= 1:\n",
        "        seperated_docs.append(curr_doc)\n",
        "    return seperated_docs\n",
        "\n",
        "\n",
        "  def create_permed_masked_input(self, min_pred_len: int=10):\n",
        "    for first_idx, second_idx in self.sampled_segment_pair:\n",
        "      input_len = len(self.text_segment[first_idx]) + len(self.text_segment[second_idx]) + 3\n",
        "\n",
        "      perm_mask = np.zeros((input_len, input_len))\n",
        "\n",
        "      self.perm_order = np.arange(input_len)\n",
        "\n",
        "      np.random.shuffle(self.perm_order)\n",
        "\n",
        "      c_stream_perm_mask = perm_mask\n",
        "\n",
        "      q_stream_perm_mask = perm_mask\n",
        "\n",
        "      output_perm_mask = perm_mask\n",
        "\n",
        "      count = 0\n",
        "      for i in self.perm_order[input_len - min_pred_len:]:\n",
        "        c_stream_perm_mask[count:, i] = 1\n",
        "        count += 1\n",
        "      \n",
        "      c_stream_perm_mask[:,self.perm_order[:,:input_len - min_pred_len]] = 1\n",
        "\n",
        "      count = 0\n",
        "      for i in self.perm_order[input_len - min_pred_len - 1:-1]:\n",
        "        q_stream_perm_mask[count:, i] = 1\n",
        "        count += 1\n",
        "      \n",
        "      q_stream_perm_mask[:,self.perm_order[:,:input_len - min_pred_len - 1]] = 1\n",
        "      \n",
        "\n",
        "      for mask_until in range(min_pred_len - 1, curr_reuse_len + curr_first_seg_len + curr_second_seg_len + 3)\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input[idx]\n",
        "\n",
        "      #sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_JZZiprKAhf"
      },
      "source": [
        "aaa = Xlnet_Input([[te_dat.examples[0].text][0]], Field = TEXT, sep_docs=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUph0ZjES2J2"
      },
      "source": [
        "aaa.create_xlnet_input(128,128,259)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnGzrfClTOfL",
        "outputId": "a180b306-5658-4911-dd47-ff6c3e071602"
      },
      "source": [
        "len(aaa.input[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y4_L89BlaI8",
        "outputId": "d561a49c-59aa-4df2-ddfb-e1ea7a8b7f38"
      },
      "source": [
        "aaa.input[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " (128, 128, 128),\n",
              " (tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
              "          142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "          156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
              "          170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
              "          184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
              "          198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
              "          212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
              "          226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
              "          240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
              "          254, 255]),\n",
              "  tensor([260992, 260993, 260994, 260995, 260996, 260997, 260998, 260999, 261000,\n",
              "          261001, 261002, 261003, 261004, 261005, 261006, 261007, 261008, 261009,\n",
              "          261010, 261011, 261012, 261013, 261014, 261015, 261016, 261017, 261018,\n",
              "          261019, 261020, 261021, 261022, 261023, 261024, 261025, 261026, 261027,\n",
              "          261028, 261029, 261030, 261031, 261032, 261033, 261034, 261035, 261036,\n",
              "          261037, 261038, 261039, 261040, 261041, 261042, 261043, 261044, 261045,\n",
              "          261046, 261047, 261048, 261049, 261050, 261051, 261052, 261053, 261054,\n",
              "          261055, 261056, 261057, 261058, 261059, 261060, 261061, 261062, 261063,\n",
              "          261064, 261065, 261066, 261067, 261068, 261069, 261070, 261071, 261072,\n",
              "          261073, 261074, 261075, 261076, 261077, 261078, 261079, 261080, 261081,\n",
              "          261082, 261083, 261084, 261085, 261086, 261087, 261088, 261089, 261090,\n",
              "          261091, 261092, 261093, 261094, 261095, 261096, 261097, 261098, 261099,\n",
              "          261100, 261101, 261102, 261103, 261104, 261105, 261106, 261107, 261108,\n",
              "          261109, 261110, 261111, 261112, 261113, 261114, 261115, 261116, 261117,\n",
              "          261118, 261119])),\n",
              " (1, 2039),\n",
              " (0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqGGP3YhirRM"
      },
      "source": [
        "dd = []\n",
        "dd.append([0] * 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riQogoHaTSnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwDhVekJi11M",
        "outputId": "e150a831-d92a-4995-b2c4-02edda8e0f85"
      },
      "source": [
        "[0,1,2][:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOFMa_6LYNQZ",
        "outputId": "250f031f-4e57-49d4-9f4e-23cd0a31083d"
      },
      "source": [
        "aaa.Field.vocab.vectors[aaa.Field.vocab.stoi.get(\"<sep>\")]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC292mA7bwVv",
        "outputId": "4a34bfeb-15b4-4d9a-bd6f-d75b0d97821d"
      },
      "source": [
        "aaa.target_text[0:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYaFti9rfdpX",
        "outputId": "4b8f2fe8-08ad-4bac-a6a3-fed99dc1ba53"
      },
      "source": [
        "aaa.text_segment[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-NAwlLcRYT",
        "outputId": "1c573ec9-cb87-468f-f6a5-55d03cb0a754"
      },
      "source": [
        "aaa.target_text[3].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTYCjHlSdCAA",
        "outputId": "28aa90f6-907c-40d0-8b7e-027e45b388fc"
      },
      "source": [
        "aaa.text_segment[3].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM1qrCdOq8NC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f99ac96a-eee5-4b37-d816-8c4290d169db"
      },
      "source": [
        "self.target_csv = pd.read_csv(traget_csv)\n",
        "    self.dat_shape = data.shape\n",
        "    self.root_dir = root_dir\n",
        "    self.segment_len = segment_len\n",
        "    self.word_dict = False\n",
        "    self.data = data\n",
        "\n",
        "    if do_shuffle:\n",
        "      shuffle()\n",
        "    \n",
        "    segmentation()\n",
        "\n",
        "  def segmentation(self):\n",
        "    self.segments = \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    doc_paths = os.path.join(self.root_dir, self.target_csv.iloc[idx, 0])\n",
        "    for doc_path in doc_paths\n",
        "    doc_file = open(doc_path, 'r')\n",
        "\n",
        "    lines = doc_file.readlines()\n",
        "\n",
        "    if word_dict:\n",
        "      for line in lines:\n",
        "        for \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    doc_file.close()\n",
        "\n",
        "\n",
        "    f = open(\"C:/doit/새파일.txt\", 'r')\n",
        "\n",
        "\n",
        "      landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "      landmarks = np.array([landmarks])\n",
        "      landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "      sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<eos>  ='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB1W23lzrcgC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv5OFVg2XxFH"
      },
      "source": [
        "aa = [tr_dat.examples[0].text][0][:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwJmxL-iare"
      },
      "source": [
        "result = []\n",
        "docs = (''.join(aa)).split('<eos>  =')\n",
        "for i in temp:\n",
        "  curr_doc = (''.join(i)).split('  ')\n",
        "  curr_doc = [i for i in curr_doc if i != '']\n",
        "  result.append(curr_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMDNrB8iYRjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3656384b-d915-4a69-d3f1-f7d9f982dbe4"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,    29,  4031,  2997,   447,  3285,     4,   133,   444,    36,\n",
              "         3745,     3,   838,     6, 24073,  1484,     4,    25, 13603,    71])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKr0gTe1ZL5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0421253f-8def-4ee5-bcac-c2a75ceefc04"
      },
      "source": [
        "train_data[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    7,  1781,    20,   179, 15130,   765,   632,    59,   490,  6997,\n",
              "           27,     7,     6,     4,   480,  5051,    60,    15,     8,     9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvk-s2rAYE3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d55e501-8c7b-4ff5-a964-c6664ea3c13b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Field.build_vocab of <torchtext.data.field.Field object at 0x7f1ca121d400>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt9bcA38QVly"
      },
      "source": [
        "en_textfield.build_vocab(tr_dat, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usTUPO0SSHH0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RRFhrAJuhLT"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (tr_dat, val_dat, test_dat),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65YpZ6lRctRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35f9ac9-d473-41b0-a6d6-8a19fd8ebd2d"
      },
      "source": [
        "bb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   9,   11, 3875,  ...,    4,    9,    9]]), tensor([2088628]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwjRwKvDVZE6"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvAhDzXVo4x"
      },
      "source": [
        "ds = tfds.load('glue', split='train', as_supervised=False, shuffle_files=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGywEswRWqYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d6a25df3-bf9a-46fd-bbf6-4935f1c62d69"
      },
      "source": [
        "for i in ds:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cc2ab6e2e158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1LN41CIVcxW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "9891165d-ad56-4226-983b-94fd97eea533"
      },
      "source": [
        "aa.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-603b00bbb490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Glue' has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA54tjsGB73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "615a630b-f343-43e0-ef74-bdd72e2cf185"
      },
      "source": [
        "np.array([1,2,3]).shape()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a69473d3b56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFIb0Ta-9pt"
      },
      "source": [
        "ddd = aa(a = np.array([1,2,4,5]),b=np.array([1,2,4,5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZZQirEK_0-H"
      },
      "source": [
        "exec('b=ddd.a.sum()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "norvzpPi_0Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "94505df7-9556-490d-91cc-cee5ed60ee07"
      },
      "source": [
        "len(np.array([1,2,3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8neGAK_MSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "59268c2b-827c-4152-aa90-5485f0d97107"
      },
      "source": [
        "sample_dict.get('aa')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qisDhWYI_Agt"
      },
      "source": [
        "sample_dict = {'aa': 12, 'bb':13}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3gvdyAs-6mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "176553f5-8ace-4c86-abde-e8c0fea3275a"
      },
      "source": [
        "'list is list'.split(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list', 'is', 'list']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghK98IUDdOeV"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c2UthJ4dXb7"
      },
      "source": [
        "#활용된 잡지식\n",
        "\n",
        "#pytorch dataset 불러오기 \n",
        "#https://pytorch.org/text/datasets.html\n",
        "\n",
        "#자동으로 추가해야할 내용들\n",
        "#To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference\n",
        "\n",
        "#데이터 로딩\n",
        "#data loading\n",
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html\n",
        "\n",
        "#함수 오버라이딩 방법\n",
        "#super(MyModel, self).__init__()\n",
        "\n",
        "#einstein summation\n",
        "#https://rockt.github.io/2018/04/30/einsum\n",
        "\n",
        "#defining method with string\n",
        "#https://stackoverflow.com/questions/11553721/using-a-string-variable-as-a-variable-name\n",
        "\n",
        "#kwargs,args\n",
        "#https://brunch.co.kr/@princox/180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLLgpI56dZnK"
      },
      "source": [
        "# Two-stream attention cell\n",
        "#input : q_size, k_size, v_size, segment_index, attention_size, heads =8, **kwargs = {init_Qstream_q_LT_weight, init_Qstream_q_K_weight,init_Qstream_q_R_weight, init_Qstream_k_E_LT_weight, init_Qstream_k_R_LT_weight, init_Qstream_v_LT_weight,\n",
        "#                                                                            init_Qstream_segment_same_weight, init_Qstream_segment_notsame_weight, init_Qstream_segment_bias, init_Qstream_output_weight,\n",
        "#                                                                            init_Cstream_q_LT_weight, init_Cstream_q_K_weight,init_Cstream_q_R_weight, init_Cstream_k_E_LT_weight, init_Cstream_k_R_LT_weight, init_Cstream_v_LT_weight,\n",
        "#                                                                            init_Cstream_segment_same_weight, init_Cstream_segment_notsame_weight, init_Cstream_segment_bias, init_Cstream_output_weight}\n",
        "#\n",
        "\n",
        "#operation : q_LT, k_LT, v_LT, segmentation, permutation(optional), masking, relative_position_encoding, segment_encoding, cal_content_attention, cal_query_attention, memorise\n",
        "\n",
        "#properties : Qstream_q_LT_weight, Qstream_q_K_weight,Qstream_q_R_weight, Qstream_k_E_LT_weight, Qstream_k_R_LT_weight, Qstream_v_LT_weight,\n",
        "#             Qstream_segment_same_weight, Qstream_segment_notsame_weight, Qstream_segment_bias, Qstream_output_weight,\n",
        "#             Cstream_q_LT_weight, Cstream_q_K_weight,Cstream_q_R_weight, Cstream_k_E_LT_weight, Cstream_k_R_LT_weight, Cstream_v_LT_weight,\n",
        "#             Cstream_segment_same_weight, Cstream_segment_notsame_weight, Cstream_segment_bias, Cstream_output_weight content_attention, query_attention, content_memory, query_memory\n",
        "\n",
        "#output : Dq vector(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9SPAirbqb1a"
      },
      "source": [
        "class create_PLM_input(data_utils.Dataset):\n",
        "  def __init__(self, data, target_csv, word_dict = False, sep_by_lines = False, segment_len = False, do_shuffle=True):\n",
        "    super(create_PLM_input, self).__init__()\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4950byzWJBsJ"
      },
      "source": [
        "class create_PLM_input_inmemory(data_utils.Dataset):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edo49bgpmrCX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e768e5b7-29bf-443e-b8cc-16209fbd144f"
      },
      "source": [
        "# https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "perred\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V_hdr4sDpfz"
      },
      "source": [
        "a= torch.nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS8PYlKKEmbS"
      },
      "source": [
        "def shuffle()\n",
        "\n",
        "def permutate()\n",
        "\n",
        "def rel_position_encoding():\n",
        "\n",
        "def masking():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Fa0rBOEpbf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQmmsxUrmp8q"
      },
      "source": [
        "if not a:\n",
        "  print(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHFApw4UPpar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "6a74a673-3317-4367-fcb9-72efeaa73f9a"
      },
      "source": [
        "class Two_stream_attention(nn):\n",
        "  def __init__(self, q_size, k_size, v_size, attention_size = False, heads =8, **kwargs):\n",
        "    super(Two_stream_attention, self).__init__() \n",
        "      \n",
        "\n",
        "    self.segment_index = segment_index\n",
        "\n",
        "    #initialize params\n",
        "    self.initialize_params(self, q_size, k_size, v_size, attention_size, heads)\n",
        "\n",
        "  def initialize_params(self, q_size, k_size, v_size, attention_size, heads):\n",
        "    self.Qstream_q_LT_weight = nn.Linear()\n",
        "    self.Qstream_q_K_weight = \n",
        "    self.Qstream_q_R_weight = \n",
        "    \n",
        "    self.Qstream_k_E_LT_weight = \n",
        "    self.Qstream_k_R_LT_weight = \n",
        "    self.Qstream_v_LT_weight = \n",
        "    \n",
        "    self.Qstream_segment_same_weight = \n",
        "    self.Qstream_segment_notsame_weight = \n",
        "    self.Qstream_segment_bias = \n",
        "    \n",
        "    self.Qstream_output_weight = \n",
        "\n",
        "    self.Cstream_q_LT_weight = \n",
        "    self.Cstream_q_K_weight = \n",
        "    self.Cstream_q_R_weight = \n",
        "    \n",
        "    self.Cstream_k_E_LT_weight = \n",
        "    self.Cstream_k_R_LT_weight = \n",
        "    self.Cstream_v_LT_weight = \n",
        "\n",
        "    self.Cstream_segment_same_weight = \n",
        "    self.Cstream_segment_notsame_weight = \n",
        "    self.Cstream_segment_bias = \n",
        "    \n",
        "    self.Cstream_output_weight =\n",
        "\n",
        "  def set_params(self,**kwargs):\n",
        "    for i,j in kwargs.items():\n",
        "      setattr(self, i, j)\n",
        "      explicitly_defined_vars.append(i)\n",
        "\n",
        "  def forward(self, input, do_parm = False, rel_position = False):\n",
        "    \n",
        "    #record position\n",
        "    if not rel_position:\n",
        "      self.rel_position = rel_position_encoding(input)\n",
        "    else:\n",
        "      self.rel_position = rel_position\n",
        "\n",
        "    #permutation\n",
        "    if do_parm:\n",
        "      self.permutate()\n",
        "\n",
        "    #get position encoding\n",
        "\n",
        "    #cal\n",
        "  def backward(self,??): #클레스 정의하는 법 참조\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ad5720c06428>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    self.Qstream_q_K_weight =\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laXkR4scazxZ"
      },
      "source": [
        "_#http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxPfane-X3-A"
      },
      "source": [
        "##Copyright 2013, Youngbin Jang, All rights reserved."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grV-X_-sfTcW"
      },
      "source": [
        "#https://tutorials.pytorch.kr/beginner/nn_tutorial.html"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}